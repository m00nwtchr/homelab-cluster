---
# yaml-language-server: $schema=https://k8s-schemas.m00nlit.dev/source.toolkit.fluxcd.io/ocirepository_v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: vm-k8s-stack
spec:
  interval: 5m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: 0.68.0
  url: oci://ghcr.io/victoriametrics/helm-charts/victoria-metrics-k8s-stack
  # TODO: Flux v2.8
  # verify:
  #   provider: cosign
  #   matchOIDCIdentity:
  #     - issuer: '^https://token.actions.githubusercontent.com$'
  #       subject: '^https://github.com/victoriametrics/helm-charts.*$'
---
# yaml-language-server: $schema=https://k8s-schemas.m00nlit.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app vmks
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: vm-k8s-stack
  dependsOn:
    - name: victoria-metrics-operator
  values:
    global:
      cluster:
        # -- K8s cluster domain suffix, uses for building storage pods' FQDN. Details are [here](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/)
        dnsDomain: cluster.local.
    nameOverride: *app

    victoria-metrics-operator:
      enabled: false

    defaultDashboards:
      enabled: true
      grafanaOperator:
        enabled: true
        spec:
          instanceSelector:
            matchLabels:
              dashboards: null
              grafana.internal/instance: grafana

    defaultDatasources:
      grafanaOperator:
        # -- Create datasources as CRDs (requires grafana-operator to be installed)
        enabled: true
        spec:
          instanceSelector:
            matchLabels:
              dashboards: null
              grafana.internal/instance: grafana

    grafana:
      enabled: false

    defaultRules:
      create: true

    alertmanager:
      enabled: false
      spec:
        configReloaderExtraArgs:
          enableTCP6: "true"
        externalURL: https://alertmanager.${SECRET_DOMAIN}
        storage:
          volumeClaimTemplate:
            spec:
              resources:
                requests:
                  storage: 1Gi

    vmalert:
      enabled: false
      spec:
        extraArgs:
          enableTCP6: "true"
        configReloaderExtraArgs:
          enableTCP6: "true"

    vmsingle:
      spec:
        extraArgs:
          enableTCP6: "true"
          maxLabelsPerTimeseries: "40"
          search.maxUniqueTimeseries: "600000"
          search.minStalenessInterval: 5m
        replicaCount: 1
        retentionPeriod: 30d
        storage:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 60Gi
        useDefaultResources: false
        useStrictSecurity: true
        resources:
          limits:
            memory: 2Gi
            cpu: 1
          requests:
            cpu: 100m

    vmagent:
      spec:
        extraArgs:
          enableTCP6: "true"
        configReloaderExtraArgs:
          enableTCP6: "true"
        resources:
          limits:
            cpu: 2000m
            memory: 1Gi
        selectAllByDefault: true
        scrapeInterval: 20s

    prometheus-node-exporter:
      enabled: true
      fullnameOverride: node-exporter

    kube-state-metrics:
      enabled: true
      fullnameOverride: kube-state-metrics

    kubelet:
      vmScrapes:
        kubelet:
          enabled: true
      vmScrape:
        spec:
          port: "10250"
          relabelConfigs:
            # This is the same as the chart default, but it explicitly excludes NFD node labels,
            # and talos extension labels, and some others. These don't really provide any value,
            # and they cause cardinality to explode.
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - action: labeldrop
              regex: (feature_node|extensions_talos_dev|beta_kubernetes|plan_upgrade_cattle_io|openebs_io|katacontainers_io|topology_rook_io_).*
            - action: labeldrop
              regex: .*kubevirt_io.*
            - action: labeldrop
              regex: .*home_arpa.*
            - sourceLabels: [__metrics_path__]
              targetLabel: metrics_path
            - targetLabel: job
              replacement: kubelet

    kubeApiServer:
      enabled: true

    kubeControllerManager:
      enabled: false

    kubeEtcd:
      service:
        selector:
          # This is a workaround for targeting etcd instance on all control planes.
          # Because etcd does not run in a pod, it cannot be directly targeted via a service.
          # Instead, the service must select pods that run on the same nodes as the etcd instances,
          # and also have host networking enabled. The controller manager and scheduler are good
          # choices because they meet these requirements.
          k8s-app: cilium
          component: ~
        # This port does not require authentication, and only serves metrics. TODO build automation
        # to pull in the etcd certs via talosctl, which can be called via pods with a Talos service account.
        # Then the normal, authenticated 2379 port can be used. Or even better, move the root CA out
        # of the cluster (probably into a HSM), and issue these certs with it.
        targetPort: 2381
      vmScrape:
        spec:
          endpoints:
            - port: http-metrics
              scheme: http

    kubeScheduler:
      enabled: false

    kubeProxy:
      enabled: false

    coreDns:
      enabled: false
